{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>  Build agentic AI application using Langgraph </h1>\n",
    "<p> Built by Jakob Lindstr√∏m, aka DataJakob@Github,  for Lumos  SDC. </p>\n",
    "<p> In this notebook  we will create a agentic chatbot that is especially good at Q&A for finance and portfolio generation </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1 Import libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment \n",
    "import os\n",
    "\n",
    "# \"Lang\" packages\n",
    "import langchain\n",
    "import langgraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "# Other\n",
    "from demo.analyze.data import PortofolioCloud\n",
    "from demo.analyze.optimizer import Optimized\n",
    "\n",
    "# .env variables\n",
    "# my_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2 State and classes </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    mission: str\n",
    "    company_info: str\n",
    "    stocks: Annotated[list, add_messages]\n",
    "    optimal_portfolio: str = Field(description=\"\"\"A sentence describing the optimal stock posistion in a portfolio\n",
    "                                                based on the Sharpe ratio\"\"\")\n",
    "\n",
    "\n",
    "class Mission(BaseModel):\n",
    "    mission: str = Field(description=\"\"\"\n",
    "                         Answer with one word based on the user query:\n",
    "                         - \"append\", if the user wants to append  the stock into the portfolio.\n",
    "                         - \"more_info\"  if the user wants more info about a stock or something else.\n",
    "                         - \"analyze\"  if  the user wants to analyze the current portfolio,\n",
    "                         - \"other\", if nothing of the above is specified.\n",
    "                        \"\"\")\n",
    "    \n",
    "class Filter(BaseModel):\n",
    "    ticker: str = Field(description=\"\"\"\n",
    "                        Identify the financial ticker of the mentioned company. \n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 Model and Nodes </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=my_api_key\n",
    ")\n",
    "\n",
    "\n",
    "def chatbot(system_message, user_message):\n",
    "    system_input = \"You are chatbot specializing in financial advisory. \" + str(system_message)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_input),\n",
    "    ])\n",
    "    info_model = prompt | llm \n",
    "    response =  info_model.invoke({\"user\":user_message})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator(state: State):\n",
    "    nav_model =  llm.with_structured_output(Mission)\n",
    "    response = nav_model.invoke(state[\"messages\"]).mission\n",
    "    return {\"mission\": response}\n",
    "\n",
    "\n",
    "\n",
    "def info(state: State):\n",
    "    response = chatbot(\"You shall give a short description of what a company does based on the user input.\",\n",
    "                       state[\"messages\"]\n",
    "                       )\n",
    "    return {\"company_info\": response.content}\n",
    "\n",
    "\n",
    "    \n",
    "def stock_appender(state: State):\n",
    "    nav_model =  llm.with_structured_output(Filter)\n",
    "    response = nav_model.invoke(state[\"messages\"]).ticker+\".OL\"\n",
    "    print(state[\"stocks\"])\n",
    "    return {\"stocks\": response}\n",
    "\n",
    "\n",
    "\n",
    "def portfolio_analyzer(state: State):\n",
    "    stocks = [x.content for x in state[\"stocks\"]]\n",
    "    print(stocks)\n",
    "    alfa = PortofolioCloud(stocks)\n",
    "    alfa.final_df()\n",
    "    bravo = Optimized(alfa.stocks, alfa.df,\n",
    "                    alfa.tot_cov_mat,alfa.neg_cov_mat,\n",
    "                    alfa.returns, alfa.mean, alfa.std)\n",
    "    bravo.PortofolioOptimizer()\n",
    "    positions =  bravo.optimized_portofolios\n",
    "    myList = \"\"\n",
    "    for i in range(0, len(stocks),1):\n",
    "        myList += str(stocks[i]+ \" at \" +str(round(positions[2][0][i]*100,1))+\"%, \")\n",
    "    print(myList)\n",
    "\n",
    "    return {\"optimal_portfolio\": myList}\n",
    "\n",
    "\n",
    "\n",
    "def final_response(state: State):\n",
    "    if state[\"mission\"] == \"other\":\n",
    "        response = chatbot(\"\"\"The user has given you a unrelated query, make the user give queries about appending stocks to a porfolio,\n",
    "                           get information about a company or analyze a portfolio \"\"\",\n",
    "                           state[\"messages\"]\n",
    "                           )\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    elif state[\"mission\"] == \"analyze\":\n",
    "        response = chatbot(\"\"\"Use the input from the user to inform the user about the stock posistion that would generate\n",
    "                           the best portfolio based on the Sharpe ratio.\"\"\",\n",
    "                            state[\"optimal_portfolio\"]\n",
    "                            )\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    elif state[\"mission\"] == \"append\":\n",
    "        response = chatbot(\"\"\"You have just added a stock to a portfolio, ask the user if a new stock should be added.\n",
    "                            Or if the user wants to get info about a company or  analyze the portfolio\"\"\",\n",
    "                            state[\"messages\"]\n",
    "                            )\n",
    "        return {\"messages\": response}\n",
    "        \n",
    "    else:\n",
    "        response = chatbot(\"\"\" Give key pieces of information about a company that the user have requested.\"\"\",\n",
    "                           state[\"messages\"]\n",
    "                           )\n",
    "        return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4 Edges</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(state: State):\n",
    "    if state[\"mission\"] == \"append\":\n",
    "        return \"add_stock\"\n",
    "    elif state[\"mission\"] == \"analyze\":\n",
    "        return  \"analyze_portfolio\"\n",
    "    elif state[\"mission\"] == \"info\":\n",
    "        return \"more_info\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5 Workflow Compilation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"nav\", navigator)\n",
    "workflow.add_node(\"info\", info)\n",
    "workflow.add_node(\"append\", stock_appender)\n",
    "workflow.add_node(\"analyze\", portfolio_analyzer)\n",
    "workflow.add_node(\"response\", final_response)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"nav\")     # Non-optional move\n",
    "workflow.add_conditional_edges(\"nav\",   # Starting node\n",
    "                               decision,    # Decider for which node to go to\n",
    "                               # Options\n",
    "                               {\"more_info\":\"info\",\n",
    "                                \"add_stock\":\"append\",\n",
    "                                \"analyze_portfolio\":\"analyze\",\n",
    "                                \"other\":\"response\"}\n",
    "\n",
    ")\n",
    "workflow.add_edge(\"info\", \"response\")\n",
    "workflow.add_edge(\"append\", \"response\")\n",
    "workflow.add_edge(\"analyze\", \"response\")\n",
    "workflow.add_edge(\"response\", END)\n",
    "\n",
    "# Compile and save the graph\n",
    "# workflow.compile()\n",
    "mygraph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6 Start chatting :) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: nav\n",
      "['EQNR.OL', 'MOWI.OL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Jakob\\OneDrive\\Dokumenter\\master\\Lumos\\langgraph\\.venvLSDC\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:316: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQNR.OL at 48.1%, MOWI.OL at 51.9%, \n",
      "Assistant: analyze\n",
      "Assistant: response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: nav\n",
      "['EQNR.OL', 'MOWI.OL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Jakob\\OneDrive\\Dokumenter\\master\\Lumos\\langgraph\\.venvLSDC\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:316: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQNR.OL at 48.1%, MOWI.OL at 51.9%, \n",
      "Assistant: analyze\n",
      "Assistant: response\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in mygraph.stream({\"messages\": [(\"user\", user_input)]},\n",
    "                                {\"configurable\": {\"thread_id\": \"0a\"}}):\n",
    "        for value in event:\n",
    "            print(\"Assistant:\", value)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"----Something wong here------\"    \n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvLSDC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
